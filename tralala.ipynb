{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Place where I test petl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API => CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import petl as etl\n",
    "\n",
    "api_url = 'https://swapi.dev/api/people'\n",
    "response = requests.get(api_url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    json_data = response.json()['results']\n",
    "    table = etl.fromdicts(json_data)\n",
    "    etl.tocsv(table, 'output.csv')\n",
    "    print(\"CSV file created successfully.\")\n",
    "else:\n",
    "    print(\"Failed to fetch data from the API.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API * all_pages => CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import petl as etl\n",
    "\n",
    "def fetch_all_characters(api_url):\n",
    "    all_characters = []\n",
    "    while api_url:\n",
    "        response = requests.get(api_url)\n",
    "        if response.status_code == 200:\n",
    "            json_data = response.json()\n",
    "            all_characters.extend(json_data['results'])\n",
    "            api_url = json_data['next']\n",
    "        else:\n",
    "            print(f\"Failed to fetch data from the API. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    return all_characters\n",
    "\n",
    "initial_api_url = 'https://swapi.dev/api/people/'\n",
    "characters = fetch_all_characters(initial_api_url)\n",
    "\n",
    "if characters:\n",
    "    table = etl.fromdicts(characters)\n",
    "    etl.tocsv(table, 'star_wars_characters_list.csv')\n",
    "    print(\"CSV file created successfully.\")\n",
    "else:\n",
    "    print(\"No characters fetched from the API.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving to class, separating checking for status code, added MAX_PAGES to avoid for the code to work indefinietely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import petl as etl\n",
    "\n",
    "class SWAPIHandler:\n",
    "    MAX_PAGES = 10\n",
    "\n",
    "    def __init__(self):\n",
    "        self.base_url = 'https://swapi.dev/api/people/'\n",
    "\n",
    "    def _handle_response(self, response):\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch data from the API. Status code: {response.status_code}\")\n",
    "            return None\n",
    "        return response.json()\n",
    "\n",
    "    def fetch_all_characters(self):\n",
    "        all_characters = []\n",
    "        api_url = self.base_url\n",
    "        page_count = 0\n",
    "        while api_url and page_count < self.MAX_PAGES:\n",
    "            response = requests.get(api_url)\n",
    "            json_data = self._handle_response(response)\n",
    "            if not json_data:\n",
    "                return None\n",
    "            all_characters.extend(json_data['results'])\n",
    "            api_url = json_data['next']\n",
    "            page_count += 1\n",
    "        return all_characters\n",
    "\n",
    "class TestSWAPIHandler(SWAPIHandler):\n",
    "    def export_to_csv(self, filename):\n",
    "        characters = self.fetch_all_characters()\n",
    "        if characters:\n",
    "            table = etl.fromdicts(characters)\n",
    "            etl.tocsv(table, filename)\n",
    "            print(\"CSV file created successfully.\")\n",
    "        else:\n",
    "            print(\"No characters fetched from the API.\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "exporter = TestSWAPIHandler()\n",
    "exporter.export_to_csv(\"output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Logic behind main points:\n",
    "1. Data cleaning inside a loop - In the requirements of this task was stated\n",
    "that I need to keep memory footprint as low as possible. Therefore even tasks\n",
    "with O(1) complexity are done inside a loop to avoid storing all data in memory,\n",
    "which increases redundancy.\n",
    "2. MAX_PAGES - Simple approach to limit the number of pages fetched from the API.\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import petl as etl\n",
    "\n",
    "class SWAPIHandler:\n",
    "    MAX_PAGES = 10\n",
    "    BASE_URL = 'https://swapi.dev/api/people/'\n",
    "    \n",
    "    def handle_swapi(self):\n",
    "        page_count = 0\n",
    "        api_url = self.BASE_URL\n",
    "        all_data = []\n",
    "\n",
    "\n",
    "        while api_url and page_count < self.MAX_PAGES:\n",
    "            response = requests.get(api_url)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()['results']\n",
    "\n",
    "            all_data.extend(data)\n",
    "\n",
    "            api_url = response.json()['next']\n",
    "            page_count += 1\n",
    "\n",
    "        table = etl.fromdicts(all_data)\n",
    "        cleaned_data = self.clean_data(table)\n",
    "        \n",
    "        return cleaned_data\n",
    "\n",
    "    def clean_data(self, data):\n",
    "        cleaned_data = etl.cutout(\n",
    "            data, \n",
    "            'films', \n",
    "            'species', \n",
    "            'vehicles', \n",
    "            'starships', \n",
    "            'created', \n",
    "            'edited'\n",
    "            )\n",
    "\n",
    "        \n",
    "        return cleaned_data\n",
    "    \n",
    "class TestSWAPIHandler(SWAPIHandler):\n",
    "    def convert_to_csv(self):\n",
    "        table = self.handle_swapi()\n",
    "        csv_buffer = \"output.csv\"\n",
    "        etl.tocsv(table, csv_buffer)\n",
    "\n",
    "\n",
    "exporter = TestSWAPIHandler()\n",
    "exporter.convert_to_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Logic behind main points:\n",
    "1. Data cleaning inside a loop - In the requirements of this task was stated\n",
    "that I need to keep memory footprint as low as possible. Therefore even tasks\n",
    "with O(1) complexity are done inside a loop to avoid storing all data in memory,\n",
    "which increases redundancy.\n",
    "2. MAX_PAGES - Simple approach to limit the number of pages fetched from the API.\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import petl as etl\n",
    "\n",
    "\n",
    "class SWAPIHandler:\n",
    "    MAX_PAGES = 10\n",
    "    BASE_URL = 'https://swapi.dev/api/people/'\n",
    "    planet_dict : dict[str, str] = {}\n",
    "\n",
    "    def handle_swapi(self):\n",
    "        page_count = 0\n",
    "        api_url = self.BASE_URL\n",
    "        all_data = []\n",
    "\n",
    "\n",
    "        while api_url and page_count < self.MAX_PAGES:\n",
    "            response = requests.get(api_url)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()['results']\n",
    "\n",
    "            all_data.extend(data)\n",
    "\n",
    "            api_url = response.json()['next']\n",
    "            page_count += 1\n",
    "\n",
    "        table = etl.fromdicts(all_data)\n",
    "        cleaned_data = self.clean_data(table)\n",
    "\n",
    "        return cleaned_data\n",
    "\n",
    "    def clean_data(self, data):\n",
    "        data = etl.convert(data, 'homeworld', lambda row: self.resolve_homeworld(row))\n",
    "        data = etl.addfield(data, 'date', lambda row: self.get_left_substring(row['edited'], 'T'))\n",
    "        data = etl.cutout(\n",
    "            data, \n",
    "            'films', \n",
    "            'species', \n",
    "            'vehicles', \n",
    "            'starships', \n",
    "            'created', \n",
    "            'edited',\n",
    "            'url'\n",
    "            )\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def get_left_substring(self, string: str, to_find: str) -> str:\n",
    "        index_of_t = string.find(to_find)\n",
    "        \n",
    "        if index_of_t != -1:\n",
    "            return string[:index_of_t]\n",
    "        else:\n",
    "            return string\n",
    "        \n",
    "    def resolve_homeworld(self, row: str) -> str:\n",
    "        if row in self.planet_dict:\n",
    "            row = self.planet_dict[row]\n",
    "        else:\n",
    "            if row.startswith('https'):\n",
    "                key = row\n",
    "                response = self.get_data_from_api(row)\n",
    "                if response:\n",
    "                    row = response['name']\n",
    "                    self.planet_dict[key] = row\n",
    "        return row\n",
    "    \n",
    "    def get_data_from_api(self, url: str) -> dict:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "class TestSWAPIHandler(SWAPIHandler):\n",
    "    def convert_to_csv(self, filename: str) -> None:\n",
    "        table = self.handle_swapi()\n",
    "        csv_buffer = filename\n",
    "        etl.tocsv(table, csv_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "exporter = TestSWAPIHandler()\n",
    "exporter.convert_to_csv(\"output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS:  <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "\n",
    "async def fetch_page(session, url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fetches a page.\n",
    "    \"\"\"\n",
    "    async with session.get(url) as response:\n",
    "        return await response.json()\n",
    "\n",
    "async def fetch_all_pages(start_url: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Fetches all pages.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    url = start_url\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        while url:\n",
    "            tasks.append(fetch_page(session, url))\n",
    "            url = await get_next_url(session, url)\n",
    "\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        print(\"RESULTS: \", type(results))\n",
    "        for result in results:\n",
    "            if result:\n",
    "                data.extend(result['results'])\n",
    "\n",
    "    return data\n",
    "\n",
    "async def get_next_url(session, url: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves the next URL from the current page.\n",
    "    \"\"\"\n",
    "    async with session.get(url) as response:\n",
    "        page_data = await response.json()\n",
    "        return page_data['next']\n",
    "\n",
    "async def main() -> None:\n",
    "    start_url = 'https://swapi.dev/api/people/?page=1'\n",
    "    all_data = await fetch_all_pages(start_url)\n",
    "\n",
    "    df = pd.DataFrame(all_data)\n",
    "    df.to_csv('output3.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # For Jupyter and other interactive environments\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main())\n",
    "    except RuntimeError:\n",
    "        # For standard Python scripts\n",
    "        asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Make an api call as a function with proper error handling.\n",
    "\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import logging\n",
    "import petl as etl\n",
    "\n",
    "class SWAPIHandler:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.planet_dict = {}\n",
    "        \n",
    "    async def get_data_from_api(self, url, session):\n",
    "        try:\n",
    "            async with session.get(url) as response:\n",
    "                response.raise_for_status()\n",
    "                return await response.json()\n",
    "        except aiohttp.ClientResponseError as e:\n",
    "            logging.error(f\"Error fetching data from {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    async def resolve_homeworld(self, session, url: str) -> str:\n",
    "        if url in self.planet_dict:\n",
    "            return self.planet_dict[url]\n",
    "        else:\n",
    "            if url.startswith('https'):\n",
    "                response = await self.get_data_from_api(url, session)\n",
    "                if response:\n",
    "                    planet_name = response['name']\n",
    "                    self.planet_dict[url] = planet_name\n",
    "            return planet_name  \n",
    "\n",
    "    async def clean_data(self, session, data: list[dict]) -> etl.Table:\n",
    "        data = etl.fromdicts(data)\n",
    "        data = etl.convert(\n",
    "            data, \n",
    "            'homeworld', \n",
    "            lambda row: self.resolve_homeworld(session, row)\n",
    "            )\n",
    "        data = etl.addfield(data, 'date', lambda row: row['edited'].split('T')[0])\n",
    "        data = etl.cutout(\n",
    "            data, \n",
    "            'films', \n",
    "            'species', \n",
    "            'vehicles', \n",
    "            'starships', \n",
    "            'created', \n",
    "            'edited',\n",
    "            'url'\n",
    "            )\n",
    "        return data\n",
    "    \n",
    "    async def get_data_and_url(self, session, url: str) -> tuple[list[dict], str]:\n",
    "        try:\n",
    "            async with session.get(url) as response:\n",
    "                response.raise_for_status()\n",
    "                swapi = await response.json()\n",
    "                data = swapi['results']\n",
    "                url = swapi['next']\n",
    "\n",
    "                return data, url\n",
    "        except aiohttp.ClientResponseError as e:\n",
    "            logging.error(f\"Error fetching data from {url}: {e}\")\n",
    "            return [], None\n",
    "        \n",
    "    async def fetch_all_pages(self, start_url: str) -> None:\n",
    "        headers_included: bool = False\n",
    "        url = start_url\n",
    "\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            while url:\n",
    "                try:\n",
    "                    data, url = await self.get_data_and_url(session, url)\n",
    "                    results = await self.clean_data(session, data)\n",
    "\n",
    "                    if not headers_included:\n",
    "                        etl.tocsv(results, 'output2.csv')\n",
    "                        headers_included = True\n",
    "                    else:\n",
    "                        etl.appendcsv(results, 'output2.csv')\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing data from {url}: {e}\")\n",
    "                    break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # For Jupyter and other interactive environments\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(SWAPIHandler().fetch_all_pages('https://swapi.dev/api/people/'))\n",
    "    except RuntimeError:\n",
    "        # For standard Python scripts\n",
    "        asyncio.run(SWAPIHandler().fetch_all_pages('https://swapi.dev/api/people/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import logging\n",
    "import petl as etl\n",
    "\n",
    "class SWAPIHandler:\n",
    "    def __init__(self):\n",
    "        self.planet_dict = {}\n",
    "        \n",
    "    async def get_data_from_api(self, url, session):\n",
    "        try:\n",
    "            async with session.get(url) as response:\n",
    "                response.raise_for_status()\n",
    "                return await response.json()\n",
    "        except aiohttp.ClientResponseError as e:\n",
    "            logging.error(f\"Error fetching data from {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    async def resolve_homeworld(self, session, url: str) -> str:\n",
    "        if url in self.planet_dict:\n",
    "            return self.planet_dict[url]\n",
    "        else:\n",
    "            if url.startswith('https'):\n",
    "                response = await self.get_data_from_api(url, session)\n",
    "                if response:\n",
    "                    planet_name = response['name']\n",
    "                    self.planet_dict[url] = planet_name\n",
    "                    return planet_name  \n",
    "            return None\n",
    "\n",
    "    async def preprocess_homeworlds(self, session, data: list[dict]) -> list[dict]:\n",
    "        for row in data:\n",
    "            if 'homeworld' in row and row['homeworld']:\n",
    "                row['homeworld'] = await self.resolve_homeworld(session, row['homeworld'])\n",
    "        return data\n",
    "    \n",
    "    def clean_data(self, data: list[dict]) -> etl.Table:\n",
    "        data = etl.fromdicts(data)\n",
    "        data = etl.addfield(data, 'date', lambda row: row['edited'].split('T')[0])\n",
    "        data = etl.cutout(\n",
    "            data, \n",
    "            'films', \n",
    "            'species', \n",
    "            'vehicles', \n",
    "            'starships', \n",
    "            'created', \n",
    "            'edited',\n",
    "            'url'\n",
    "            )\n",
    "        return data\n",
    "    \n",
    "    async def get_data_and_url(self, session, url: str) -> tuple[list[dict], str]:\n",
    "        try:\n",
    "            async with session.get(url) as response:\n",
    "                response.raise_for_status()\n",
    "                swapi = await response.json()\n",
    "                data = swapi['results']\n",
    "                url = swapi['next']\n",
    "                return data, url    \n",
    "        except aiohttp.ClientResponseError as e:\n",
    "            logging.error(f\"Error fetching data from {url}: {e}\")\n",
    "            return [], None\n",
    "        \n",
    "    async def fetch_all_pages(self, start_url: str) -> None:\n",
    "        headers_included: bool = False\n",
    "        url = start_url\n",
    "\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            while url:\n",
    "                try:\n",
    "                    data, url = await self.get_data_and_url(session, url)\n",
    "                    data = await self.preprocess_homeworlds(session, data)\n",
    "                    results = self.clean_data(data)\n",
    "\n",
    "                    if not headers_included:\n",
    "                        etl.tocsv(results, 'output2.csv')\n",
    "                        headers_included = True\n",
    "                    else:\n",
    "                        etl.appendcsv(results, 'output2.csv')\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing data from {url}: {e}\")\n",
    "                    break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # For Jupyter and other interactive environments\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(SWAPIHandler().fetch_all_pages('https://swapi.dev/api/people/'))\n",
    "    except RuntimeError:\n",
    "        # For standard Python scripts\n",
    "        asyncio.run(SWAPIHandler().fetch_all_pages('https://swapi.dev/api/people/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWAPIHandler:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.planet_dict = {}\n",
    "        \n",
    "    async def get_data_from_api(self, url, session):\n",
    "        try:\n",
    "            async with session.get(url) as response:\n",
    "                response.raise_for_status()\n",
    "                return await response.json()\n",
    "        except aiohttp.ClientResponseError as e:\n",
    "            logging.error(f\"Error fetching data from {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    async def resolve_homeworld(self, session, url: str) -> str:\n",
    "        if url in self.planet_dict:\n",
    "            return self.planet_dict[url]\n",
    "        else:\n",
    "            planet_name = None\n",
    "            if url.startswith('https'):\n",
    "                response = await self.get_data_from_api(url, session)\n",
    "                if response:\n",
    "                    planet_name = response['name']\n",
    "                    self.planet_dict[url] = planet_name\n",
    "            return planet_name\n",
    "\n",
    "    async def clean_data(self, session, data: list[dict]) -> etl.Table:\n",
    "        data = etl.fromdicts(data)\n",
    "        data = etl.convert(\n",
    "            data, \n",
    "            'homeworld', \n",
    "            lambda row: await self.resolve_homeworld(session, row['homeworld'])\n",
    "            )\n",
    "        data = etl.addfield(data, 'date', lambda row: row['edited'].split('T')[0])\n",
    "        data = etl.cutout(\n",
    "            data, \n",
    "            'films', \n",
    "            'species', \n",
    "            'vehicles', \n",
    "            'starships', \n",
    "            'created', \n",
    "            'edited', \n",
    "            'url'\n",
    "            )\n",
    "        return data\n",
    "    \n",
    "    async def get_data_and_url(self, session, url: str) -> tuple[list[dict], str]:\n",
    "        try:\n",
    "            async with session.get(url) as response:\n",
    "                response.raise_for_status()\n",
    "                swapi = await response.json()\n",
    "                data = swapi['results']\n",
    "                url = swapi['next']\n",
    "\n",
    "                return data, url\n",
    "        except aiohttp.ClientResponseError as e:\n",
    "            logging.error(f\"Error fetching data from {url}: {e}\")\n",
    "            return [], None\n",
    "        \n",
    "    async def fetch_all_pages(self, start_url: str) -> etl.MemorySource:\n",
    "        headers_included: bool = False\n",
    "        url = start_url\n",
    "        csv_buffer = etl.MemorySource()\n",
    "\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            while url:\n",
    "                try:\n",
    "                    data, url = await self.get_data_and_url(session, url)\n",
    "                    results = await self.clean_data(session, data)\n",
    "\n",
    "                    if not headers_included:\n",
    "                        etl.tocsv(results, csv_buffer, encoding='utf-8')\n",
    "                        headers_included = True\n",
    "                    else:\n",
    "                        etl.appendcsv(results, csv_buffer, encoding='utf-8')\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing data from {url}: {e}\")\n",
    "                    break\n",
    "        \n",
    "        return csv_buffer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
